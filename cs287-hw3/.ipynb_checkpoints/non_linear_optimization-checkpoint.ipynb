{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import copy\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "from scipy.io import savemat\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import Image\n",
    "from utils import *\n",
    "from envs.cart_pole_env import CartPoleEnv\n",
    "from envs.hopper_env import HopperModEnv\n",
    "from envs.cheetah_env import CheetahModEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 [30 pt] - Linear Environment\n",
    "We start with the linear environment, similar to the one on the previous homework, and we consider optimizing for a sequence of actions, comparing shooting and collocation.\n",
    "\n",
    "First, we define the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearEnv(object):\n",
    "    def __init__(self, horizon=20, multiplier=1.):\n",
    "        self.A = multiplier * 0.1 * np.array([[0.0481, -0.5049, 0.0299, 2.6544, 1.0608],\n",
    "                                 [2.3846, -0.2312, -0.1260, -0.7945, 0.5279],\n",
    "                                 [1.4019, -0.6394, -0.1401, 0.5484, 0.1624],\n",
    "                                 [-0.0254, 0.4595, -0.0862, 2.1750, 1.1012],\n",
    "                                 [0.5172, 0.5060, 1.6579, -0.9407, -1.4441]])\n",
    "        self.B = np.array([[-0.7789, -1.2076],\n",
    "                           [0.4299, -1.6041],\n",
    "                           [0.2006, -1.7395],\n",
    "                           [0.8302, 0.2295],\n",
    "                           [-1.8465, 1.2780]])\n",
    "        self.H = 20\n",
    "\n",
    "        self.dx = self.A.shape[1]\n",
    "        self.du = self.B.shape[1]\n",
    "        self.Q = np.eye(self.dx)\n",
    "        self.R = np.eye(self.du)\n",
    "        self._init_state =  np.array([-1.9613, -1.3127, 0.0698, 0.0935, 1.2494])\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, act):\n",
    "        cost = self._state.T @ self.Q @ self._state + act.T @ self.R @ act\n",
    "        state = self.A @ self._state + self.B @ act\n",
    "        self._state = state.copy()\n",
    "        return state, cost, False, {}\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self._state = state.copy()\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = self._init_state.copy()\n",
    "        return self._init_state.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = LinearEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement the non-linear optimzation algorithms. A correct implementation should \n",
    "give an optimal cost of 7.461 for both methods, and a collocation error of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [15 pt] Shooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the shooting method, we look for the sequences of actions that minimizes the total cost by directly substuting the constraints in the objective:\n",
    "\n",
    "$$ \\min_{u_0, \\dots, u_H} c(x_0, u_0) + c(f(x_0, u_0), u_1) + c(f(f(x_0, u_0), u_1) \\cdots $$\n",
    "\n",
    "\n",
    "In order to perform the optimization, we need to define the objective function to optimize.\n",
    "Fill in the code in ```eval_shooting``` which should return the cost of the trajectory\n",
    "with the specified sequences of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_shooting(env, actions):\n",
    "    \"\"\"\n",
    "    Find the cumulative cost of the sequences of actions, which has shape [horizon, action dimension].\n",
    "    Use the function step of the environment: env.step(action). It returns: next_state, cost, done,\n",
    "    env_infos.\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    actions = actions.reshape(env.H, env.du)\n",
    "    horizon = env.H\n",
    "    \n",
    "    total_cost = 0\n",
    "\n",
    "    \n",
    "    \"\"\"YOUR CODE HERE\"\"\"\n",
    "    for i in range(horizon):\n",
    "        next_state, cost, done, _ = env.step(actions[i, :])\n",
    "        total_cost += cost\n",
    "#         env.set_state(next_state)\n",
    "    \"\"\"YOUR CODE ENDS HERE\"\"\"\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined the objective function, we can use an off-the-shelf optimizer \n",
    "to find the optimal actions. In these case, we use \n",
    "[BFGS](https://docs.scipy.org/doc/scipy-0.16.0/reference/optimize.minimize-bfgs.html#optimize-minimize-bfgs),\n",
    "which is a quasi-Newton method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 7.497855\n",
      "         Iterations: 0\n",
      "         Function evaluations: 41\n",
      "         Gradient evaluations: 1\n",
      "Optimization terminated successfully.\n",
      "The optimal cost is 7.498\n"
     ]
    }
   ],
   "source": [
    "def minimize_shooting(env, init_actions=None):\n",
    "    if init_actions is None:\n",
    "        # initialization matters for convergence they gave [-1,1], i took [-0.001,0.001]\n",
    "        init_actions = np.random.uniform(low=-.001, high=.001, size=(env.H * env.du,))\n",
    "    \"\"\"YOUR CODE HERE\"\"\"\n",
    "    res = minimize(fun=lambda x:eval_shooting(env,init_actions),# Fill this with a function that returns the cumulative cost given the states and actions,\n",
    "               x0=init_actions,# Fill this with the inital actions,  \n",
    "               method='BFGS',\n",
    "               options={'xtol': 1e-6, 'disp': True, 'verbose': 2}\n",
    "              )\n",
    "\n",
    "    act_shooting = res.x\n",
    "    print(res.message)\n",
    "    print(\"The optimal cost is %.3f\" % res.fun)\n",
    "    policy_shooting = ActPolicy(env=env, \n",
    "                                actions=act_shooting\n",
    "                               )\n",
    "    return policy_shooting\n",
    "\"\"\"YOUR CODE ENDS HERE\"\"\"\n",
    "\n",
    "policy_shooting = minimize_shooting(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [15 pt] Collocation\n",
    "Now we will do the same, but for the collocation method.  In addition to the objective function, we also have to formulate the equality constraints that capture the dynamics.\n",
    "\n",
    "$$ \\min_{u_0, x_1, u_1, \\dots, x_H, u_H} c(x_0, u_0) + c(x_1, u_1) + \\cdots + c(x_H, u_H)$$\n",
    "$$\\text{s.t.:} \\quad x_{t+1} = f(x_t, u_t) \\quad \\forall t$$\n",
    "\n",
    "Fill in the code in ``eval_collocation`` and ``constraints``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_collocation(env, x):\n",
    "    \"\"\"\n",
    "    Find the cost of the sequences of actions and state that have shape [horizon, action dimension]\n",
    "    and [horizon, state_dim], respectively.\n",
    "    Use the function step of the environment: env.step(action). It returns: next_state, cost, done,\n",
    "    env_infos.\n",
    "    In order to set the environment at a specific state use the function env.set_state(state)\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    total_cost = 0\n",
    "    states, actions = x[:env.H * env.dx], x[env.H * env.dx:]\n",
    "    states = states.reshape(env.H, env.dx)\n",
    "    actions = actions.reshape(env.H, env.du)\n",
    "    horizon = env.H\n",
    "    \"\"\"YOUR CODE HERE\"\"\"\n",
    "    for i in range(horizon):\n",
    "#         env.set_state(states[i, :])\n",
    "        next_state, cost, done, _ = env.step(actions[i, :])\n",
    "        total_cost += cost\n",
    "#         env.set_state(next_state)\n",
    "\n",
    "    \"\"\"YOUR CODE ENDS HERE\"\"\"\n",
    "    return total_cost\n",
    "\n",
    "def constraints(env, x):\n",
    "    \"\"\"\n",
    "    In optimization, the equality constraints are usually specified as h(x) = 0. In this case, we would have\n",
    "    x_{t+1} - f(x_t, u_t) = 0. Here, you have to create a list that contains the value of the different\n",
    "    constraints, i.e., [x_1 - f(x_0, u_0), x_2 - f(x_1, u_1),..., x_H - f(x_{H-1}, u_{H-1})].\n",
    "    Use the function env.set_state(state) to set the state to the variable x_t.\n",
    "    Use the function step of the environment: env.step(action), which returns next_state, cost, done,\n",
    "    env_infos; to obtain x_{t+1}.\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    constraints = []\n",
    "    states, actions = x[:env.H * env.dx], x[env.H * env.dx:]\n",
    "    states = states.reshape(env.H, env.dx)\n",
    "    actions = actions.reshape(env.H, env.du)\n",
    "    horizon = env.H\n",
    "    \n",
    "\n",
    "    \"\"\"YOUR CODE HERE\"\"\"\n",
    "    for i in range(horizon):\n",
    "#         env.set_state(states[i, :])\n",
    "        next_state, cost, done, _ = env.step(actions[i, :])\n",
    "#         env.set_state(next_state)\n",
    "        constraints.append(next_state - (env.A @ states[i, :] + env.B @ actions[i, :]))\n",
    "    \"\"\"YOUR CODE ENDS HERE\"\"\"\n",
    "    return np.concatenate(constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can too use an off-the-shelf constraint optimzation algortihm, in thise case, we make use of the\n",
    "[SLQP](https://docs.scipy.org/doc/scipy-0.16.0/reference/optimize.minimize-slsqp.html#optimize-minimize-slsqp)\n",
    " algorithm, which was seen in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 7.498736948712927\n",
      "            Iterations: 1\n",
      "            Function evaluations: 142\n",
      "            Gradient evaluations: 1\n",
      "Optimization terminated successfully\n",
      "The optimal cost is 7.499\n"
     ]
    }
   ],
   "source": [
    "def minimize_collocation(env, init_states_and_actions=None):\n",
    "    if init_states_and_actions is None:\n",
    "        init_states_and_actions = np.random.uniform(low=-.001, high=.001, size=(env.H * (env.du + env.dx),))\n",
    "\n",
    "    \"\"\"YOUR CODE HERE\"\"\"\n",
    "    eq_cons = {'type': 'eq',\n",
    "               'fun' : lambda x: constraints(env, x)\n",
    "              }\n",
    "\n",
    "    res = minimize(fun=lambda x: eval_collocation(env, init_states_and_actions),# Fill this with a function that returns the cumulative cost given the states and actions,\n",
    "                   x0= init_states_and_actions,# Fill this with the initial actions,\n",
    "                   method='SLSQP', \n",
    "                   constraints=eq_cons,\n",
    "                   options={'xtol': 1e-6, 'disp': True, 'verbose': 0}\n",
    "                  )\n",
    "    print(res.message)\n",
    "    print(\"The optimal cost is %.3f\" % res.fun)\n",
    "    states_collocation, act_collocation = res.x[:env.H * env.dx], res.x[env.H * env.dx:]\n",
    "    states_collocation = states_collocation.reshape(env.H, env.dx)\n",
    "    policy_collocation = ActPolicy(env,\n",
    "                                   actions=act_collocation)\n",
    "    \"\"\"YOUR CODE ENDS HERE\"\"\"\n",
    "    return policy_collocation, states_collocation\n",
    "\n",
    "policy_collocation, states_collocation = minimize_collocation(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Quantitative Metrics ---\n",
      "Shooting Cost 7.498\n",
      "Collocation Cost 7.461\n",
      "Collocation Error 2.965\n",
      "\n",
      "\n",
      "---- Qualitative Metrics ---\n",
      "Evolution of the value of each dimension across 20 timesteps for the shooting methods.\n",
      "Both methods converge to the origin. Shooting: solid line(-);  Collocation: dashed line(--).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cost_shoot, states_shoot = rollout(env, policy_shooting)\n",
    "cost_col, states_col = rollout(env, policy_collocation)\n",
    "states_shoot, states_col = np.array(states_shoot), np.array(states_col)\n",
    "error = np.linalg.norm(states_col - np.array(states_collocation))\n",
    "ts = np.arange(states_shoot.shape[0])\n",
    "print(\"---- Quantitative Metrics ---\")\n",
    "print(\"Shooting Cost %.3f\" % cost_shoot)\n",
    "print(\"Collocation Cost %.3f\" % cost_col)\n",
    "print(\"Collocation Error %.3f\" % error)\n",
    "\n",
    "print(\"\\n\\n---- Qualitative Metrics ---\")\n",
    "print(\"Evolution of the value of each dimension across 20 timesteps for the shooting methods.\")\n",
    "print(\"Both methods converge to the origin. Shooting: solid line(-);  Collocation: dashed line(--).\")\n",
    "\n",
    "for i in range(env.dx):\n",
    "    plt.plot(ts, states_shoot[:, i], '-', ts, states_col[:, i], '--')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 [20 pt] - Stability\n",
    " A discrete-time linear system is asymptotically stable if in the presence of no input the system converges towards the zero state. In practice, this means that the absolute value of the eigenvalues of the transition matrix must be smaller than 1. If that is not the case, the system is unstable.\n",
    "\n",
    "For instance, the previous system is stable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(np.linalg.eigvals(env.A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 pt] Theoretical Question\n",
    "Consider the linear system that we currently have, i.e., $$x_{t+1} = Ax_t + B u_t$$\n",
    "and we want to minimize the quadratic cost $$ \\frac{1}{2}\\sum_t x_t Q x_t$$\n",
    "Hence, we have a linear quadratic regulator problem. Derive the gradient update for the action variables for both optimization \n",
    "methods: shooting and collocation. In the case of collocation, do not include the update due to the constraints.\n",
    "\n",
    "Explain in a few lines why the shooting method might become unstable while the collocation method does not.\n",
    "\n",
    "Refer to the pdf for reporting this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [0 pt] Empirical Behaviour\n",
    "\n",
    "Now, we test the effect that you derived and see if the theory matches the empirical behavior. We use the same environment as in the previous part, but we just scale the transiton matrix so it has some eigenvalues larger than 1. Note this is the only change with respect to the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = LinearEnv(multiplier=10.)\n",
    "np.abs(np.linalg.eigvals(env.A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_shooting = minimize_shooting(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_collocation, states_collocation = minimize_collocation(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_shoot, states_shoot = rollout(env, policy_shooting)\n",
    "cost_col, states_col = rollout(env, policy_collocation)\n",
    "states_shoot, states_col = np.array(states_shoot), np.array(states_col)\n",
    "error = np.linalg.norm(states_col - np.array(states_collocation))\n",
    "print(\"---- Quantitative Metrics ---\")\n",
    "print(\"Shooting Cost %.3f\" % cost_shoot)\n",
    "print(\"Collocation Cost %.3f\" % cost_col)\n",
    "print(\"Collocation Error %.3f\" % error)\n",
    "\n",
    "print(\"\\n\\n---- Qualitative Metrics ---\")\n",
    "print(\"Evolution of the value of each dimension across 20 timesteps for the shooting methods.\")\n",
    "print(\"The shooting method diverges, while the collocation method achieves the desired state. Shooting: solid line(-);  Collocation: dashed line(--).\")\n",
    "ts = np.arange(states_shoot.shape[0])\n",
    "for i in range(env.dx):\n",
    "    plt.plot(ts, states_shoot[:, i], '-', ts, states_col[:, i], '--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 [0 pt] - Non-linear Environments\n",
    "A nice thing of these algorithms is that they can be applied without any modification to non-linear environments such as the MuJoCo ones. For instance, here we learn a sequence of actions that leads to forward movement in the half-cheetah environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CheetahModEnv()\n",
    "init_actions = np.random.uniform(low=-.25, high=.25, size=(env.H * env.du,))\n",
    "action_shooting = minimize_shooting(env, init_actions)\n",
    "cost_shooting, states_shooting = rollout(env, action_shooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](vids/rollout.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 [30 pt] - Open-loop vs. Closed-loop\n",
    "Until now, we have been optimizing directly the sequences of actions and then applying each of the actions in the sequences \"blindly\". While this suffices in deterministic environments, in the presence of noise it does not work out well usually. Because of the stochastic transitions, the state that you encounter at a specific time-step differs from the one predicted by the optimzation problem; as a result, the action found is no longer valid. In stochastic environments, we need close loop controllers in the form of either (i) parametric policies (e.g. linear feedback controllers or neural-networks), or (ii) non-parametric policies (e.g. model predictive control).\n",
    "\n",
    "In the following, we will compare the different behaviour of open-loop and closed-loop control methods. Use the optimal cost for the action optimization methods to check the validity of your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CartPoleEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_shooting = minimize_shooting(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 pt] Policy Optimization\n",
    "\n",
    "We will start by learning a neural network policy using a shooting method. Fill in the code for ``eval_policy``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(env, policy, params):\n",
    "    \"\"\"\n",
    "    Find the cost the policy with parameters params.\n",
    "    Use the function step of the environment: env.step(action). It returns: next_state, cost, done,\n",
    "    env_infos.\n",
    "    \n",
    "    You can set the parameters of the policy by policy.set_params(params) and get the action for the current state\n",
    "    with policy.get_action(state).\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    total_cost = 0\n",
    "    horizon = env.H\n",
    "    \n",
    "    \"\"\"YOUR CODE HERE\"\"\"\n",
    "\n",
    "    \"\"\"YOUR CODE ENDS HERE\"\"\"\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_policy_shooting(env):\n",
    "    policy_shooting = NNPolicy(env.dx, env.du, hidden_sizes=(10, 10))\n",
    "    policy_shooting.init_params()\n",
    "    params = policy_shooting.get_params()\n",
    "\n",
    "    res = minimize(lambda x: eval_policy(env, policy_shooting, x),\n",
    "                   params,\n",
    "                   method='BFGS',\n",
    "                   options={'xtol': 1e-6, 'disp': False, 'verbose': 2})\n",
    "    print(res.message)\n",
    "    print(\"The optimal cost is %.3f\" % res.fun)\n",
    "    params_shooting = res.x\n",
    "    policy_shooting.set_params(params_shooting)\n",
    "    return policy_shooting\n",
    "\n",
    "policy_shooting = minimize_policy_shooting(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 pt] Model Predictive Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPCPolicy(object):\n",
    "    def __init__(self, env, horizon):\n",
    "        self.env = env\n",
    "        self.H = horizon\n",
    "        self.env = copy.deepcopy(env)\n",
    "        np.random.seed(1)\n",
    "        self.init_actions = np.random.uniform(low=-.1, high=.1, size=(horizon * env.du,))\n",
    "        \n",
    "    def get_action(self, state, timestep):\n",
    "        \"\"\"\n",
    "        Find the cost of the sequences of actions and state that have shape [horizon, action dimension]\n",
    "        and [horizon, state_dim], respectively.\n",
    "        Use the function step of the environment: env.step(action). It returns, next_state, cost, done,\n",
    "        env_infos.\n",
    "\n",
    "        In order to set the environment at a specific state use the function self.env.set_state(state)\n",
    "        \"\"\"\n",
    "        env = self.env\n",
    "        horizon = min(self.H, env.H - timestep)\n",
    "        \n",
    "        def eval_mpc(actions, state):\n",
    "            actions = actions.reshape(horizon, env.du)\n",
    "            total_cost = 0\n",
    "            \"\"\"YOUR CODE HERE\"\"\"\n",
    "           \n",
    "            \"\"\"YOUR CODE ENDS HERE\"\"\"\n",
    "            return total_cost\n",
    "\n",
    "        self.init_actions = np.random.uniform(low=-.1, high=.1, size=(horizon * env.du,))\n",
    "        res = minimize(lambda x: eval_mpc(x, state),\n",
    "               self.init_actions, \n",
    "               method='BFGS',\n",
    "               options={'xtol': 1e-6, 'disp': False, 'verbose': 2}\n",
    "              )\n",
    "        act_shooting = res.x\n",
    "        return act_shooting[:env.du]\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mpc_policy = MPCPolicy(env, env.H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "### No noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0.\n",
    "cost_act, states_act = rollout(env, action_shooting, noise)\n",
    "cost_pi, states_pi = rollout(env, policy_shooting, noise)\n",
    "cost_mpc, states_mpc = rollout(env, mpc_policy, noise)\n",
    "states_act, states_pi, states_mpc = np.array(states_act), np.array(states_pi), np.array(states_mpc)\n",
    "print(\"---- Quantitative Metrics ---\")\n",
    "print(\"Action Cost %.3f\" % cost_act)\n",
    "print(\"Policy Cost %.3f\" % cost_pi)\n",
    "print(\"MPC Cost %.3f\" % cost_mpc)\n",
    "\n",
    "print(\"\\n\\n---- Qualitative Metrics ---\")\n",
    "print(\"Evolution of the value of the angle and angular velocity of the cart-pole environment across 50 timesteps for the open-loop, policy controller, and mpc controller.\")\n",
    "print(\"All the approaches achieve the same cost and follow the same trajectory. Open-loop: solid line(-);  Policy: dashed line(--). MPC: dotted line(.)\")\n",
    "ts = np.arange(states_act.shape[0])\n",
    "plt.plot(ts, states_act[:, 2], '-', ts, states_pi[:, 2], '--', states_mpc[:, 2], '.')\n",
    "plt.plot(ts, states_act[:, 3], '-', ts, states_pi[:, 3], '--', states_mpc[:, 3], '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 1.\n",
    "cost_act, states_act = rollout(env, action_shooting, noise)\n",
    "cost_pi, states_pi = rollout(env, policy_shooting, noise)\n",
    "cost_mpc, states_mpc = rollout(env, mpc_policy, noise)\n",
    "states_act, states_pi, states_mpc = np.array(states_act), np.array(states_pi), np.array(states_mpc)\n",
    "print(\"---- Quantitative Metrics ---\")\n",
    "print(\"Action Cost %.3f\" % cost_act)\n",
    "print(\"Policy Cost %.3f\" % cost_pi)\n",
    "print(\"MPC Cost %.3f\" % cost_mpc)\n",
    "\n",
    "print(\"\\n\\n---- Qualitative Metrics ---\")\n",
    "print(\"Evolution of the value of the angle and angular velocity of the cart-pole environment across 50 timesteps for the open-loop, policy controller, and mpc controller.\")\n",
    "print(\"In the presence of noise, the open-loop controller fails to stablize the pole, while the policy and mpc controller succeed. The MPC approach achieves the best performance. Open-loop: solid line(-);  Policy: dashed line(--). MPC: dotted line(.)\")\n",
    "ts = np.arange(states_act.shape[0])\n",
    "plt.plot(ts, states_act[:, 2], '-', ts, states_pi[:, 2], '--', states_mpc[:, 2], '.')\n",
    "plt.plot(ts, states_act[:, 3], '-', ts, states_pi[:, 3], '--', states_mpc[:, 3], '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does the MPC method perform better than having a policy?\n",
    "Is there anyway we could make the performance of the policy better?\n",
    "\n",
    "Reply in no more than 5 lines in the box below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 pt] Response:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 [20 pt] - Optimization Methods\n",
    "In the previous parts, in order to optimize the collocation methods, we have used a built-in constrained optimization algorithm. However, creating our own contrained optimization solver is fairly easy given a general solver that minimizes unconstrained functions. Here, we implement two solvers by using the merit function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 pt] Merit function\n",
    "Given a standard constrained optimization problem:\n",
    "$$\\min_x g_0(x) $$\n",
    "$$\\text{s.t.:} \\quad g_i(x) \\leq 0 \\quad \\forall i$$\n",
    "$$\\quad \\quad h_j(x) = 0 \\quad \\forall j$$\n",
    "\n",
    "We can construct its *merit function* $f_\\mu$ as\n",
    "$$ f_\\mu(x) = g_0 +\\mu \\sum_i |g_i(x)|^+ + \\mu \\sum_j |h_j(x)| $$\n",
    "\n",
    "The merit function allows us to transform a constrained optimization problem to an unconstrained one that has the same optimum as $\\mu \\rightarrow \\infty$. Here, we will just solve collocation problems without any constrain on the state space. As a result, we will not have inequality constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merit_function(env, mu, x):\n",
    "    \"\"\"\n",
    "    Implementation of the merit function. We use the previously defined functions eval_collocation and constraints\n",
    "    to obtain the cost and error of the variables.\n",
    "    \n",
    "    Note: code it in a way that mu can be either a scalar or a vector\n",
    "    \"\"\"\n",
    "    cost = eval_collocation(env, x)\n",
    "    cons = constraints(env, x)\n",
    "    \n",
    "    \"\"\"YOUR CODE HERE\"\"\"\n",
    "    merit_val = # Fill this\n",
    "    \"\"\"YOUR CODE ENDS HERE\"\"\"\n",
    "    return merit_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5 pt] Penalty Formulation\n",
    "The easiest implementation is the penalty formulation. The penalty formulation iterates between finding the minimum of the merit function and increasing the scalar value of $\\mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1.5\n",
    "mu = 1\n",
    "init_states_and_actions = np.random.uniform(low=-.1, high=.1, size=(env.H * (env.du + env.dx),))\n",
    "num_iter = 5\n",
    "\n",
    "\n",
    "for i in range(num_iter):\n",
    "    \"\"\"\n",
    "    Otimization of the penalty function, which after finding the minimium for the merrit function we increase the\n",
    "    value of mu. The value of mu should be increased as specified in the lecture.\n",
    "    \"\"\"  \n",
    "    \n",
    "    \"\"\"YOUR CODE HERE\"\"\"\n",
    "    \n",
    "    mu = # Fill this\n",
    "    \n",
    "    \"\"\"YOUR CODE ENDS HERE\"\"\"\n",
    "    \n",
    "    \n",
    "    res = minimize(lambda x: merit_function(env, mu, x),\n",
    "               init_states_and_actions,\n",
    "               method='BFGS', \n",
    "               options={'xtol': 1e-6, 'disp': False, 'verbose': 2, 'maxiter':201}\n",
    "              )\n",
    "    print(\"\\nIteration %d:\"% i)\n",
    "    print(\"Value of mu %.3f\" % mu)\n",
    "    print(\"Inner optimization: %s\" % res.message)\n",
    "    print(\"Value of merit function %.3f\" % res.fun)\n",
    "    if np.linalg.norm(init_states_and_actions - res.x) < 1e-6: break\n",
    "    init_states_and_actions = res.x\n",
    "    \n",
    "    \n",
    "\n",
    "states_var_penalty, act_penalty = res.x[:env.H * env.dx], res.x[env.H * env.dx:]\n",
    "states_var_penalty = states_var_penalty.reshape(env.H, env.dx)\n",
    "act_penalty = ActPolicy(env, act_penalty) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5 pt] Dual Descent\n",
    "A better method is the dual descent formulation, which directly solves the Langrangian of the previous optimization problem:\n",
    "$$ \\max_{\\lambda_i, \\nu_j} \\min_x g_0 +\\sum_i \\lambda_ig_i(x) + \\sum_j \\nu_j h_j(x) $$\n",
    "\n",
    "The dual descent method iterates between solving the inner minimization problem and taking a gradient step on the dual variables $\\lambda_i$ and $\\nu_j$. Here, again, we omit the $g_i$ and $\\lambda_i$ terms since we do not have these constraints.\n",
    "\n",
    "However, using the merit function instead of the Lagrangian results in a more stable behavior. For this excersice, we use the merit function. In such case, the function $h_j(x)$ is $|x_{j+1} - f(x_j, u_j)|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_states_and_actions = np.random.uniform(low=-.1, high=.1, size=(env.H * (env.du + env.dx),))\n",
    "nu = 1.5 * np.ones_like(constraints(env, init_states_and_actions))\n",
    "alpha = 1\n",
    "num_iter = 5\n",
    "\n",
    "for i in range(num_iter):\n",
    "    \"\"\"\n",
    "    Otimization using dual descent, at each iteration we find the optimal for the merrit function, and then take\n",
    "    a gradient step for nu.\n",
    "    \"\"\" \n",
    "    res = minimize(lambda x: merit_function(env, nu, x),\n",
    "               init_states_and_actions,\n",
    "               method='BFGS', \n",
    "               options={'xtol': 1e-6, 'disp': False, 'verbose': 0, 'maxiter':201}\n",
    "              )\n",
    "    print(\"\\nIteration %d:\"% i)\n",
    "    print(\"Norm of nu %.3f\" % np.linalg.norm(nu))\n",
    "    print(\"Inner optimization: %s\" % res.message)\n",
    "    print(\"Value of lagrangian %.3f\" % res.fun)\n",
    "    if np.linalg.norm(init_states_and_actions - res.x) < 1e-6: break\n",
    "        \n",
    "    init_states_and_actions = res.x\n",
    "\n",
    "    \"\"\"\n",
    "    Use the function constraints(env, init_state_and_actions) and the learning rate alpha to update the\n",
    "    value of mu.\n",
    "    \"\"\"\n",
    "    \"\"\"YOUR CODE HERE \"\"\"\n",
    "    nu = # Fill this\n",
    "    \"\"\"YOUR CODE ENDS HERE\"\"\"\n",
    "\n",
    "states_var_dual_descent, act_dual_descent = res.x[:env.H * env.dx], res.x[env.H * env.dx:]\n",
    "states_var_dual_descent = states_var_dual_descent.reshape(env.H, env.dx)\n",
    "act_dual_descent = ActPolicy(env, act_dual_descent) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_penalty, states_penalty = rollout(env, act_penalty)\n",
    "cost_dual_descent, states_dual_descent = rollout(env, act_dual_descent)\n",
    "states_penalty, states_dual_descent = np.array(states_penalty), np.array(states_dual_descent)\n",
    "error_penalty = np.linalg.norm(states_penalty - np.array(states_var_penalty))\n",
    "error_dual_descent = np.linalg.norm(states_dual_descent - np.array(states_var_dual_descent))\n",
    "\n",
    "print(\"---- Quantitative Metrics ---\")\n",
    "print(\"Cost Penalty %.3f\" % cost_penalty)\n",
    "print(\"Cost Dual Descent %.3f\" % cost_dual_descent)\n",
    "\n",
    "print(\"Error Penalty %.3f\" % error_penalty)\n",
    "print(\"Error Dual Descent %.3f\" % error_dual_descent)\n",
    "\n",
    "print(\"\\n\\n---- Qualitative Metrics ---\")\n",
    "print(\"Evolution of the value of the angle and angular velocity of the cart-pole environment across 50 timesteps for the penalty and dual descent methods.\")\n",
    "print(\"Dual descent yields to slighlthly better results. Both present non-zero error on the constraints and fail to stabilize the cart-pole.  Penalty: solid line(-);  Dual descent: dashed line(--).\")\n",
    "ts = np.arange(states_penalty.shape[0])\n",
    "plt.plot(ts, states_penalty[:, 2], '-', ts, states_dual_descent[:, 2], '--')\n",
    "plt.plot(ts, states_penalty[:, 3], '-', ts, states_dual_descent[:, 3], '--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
